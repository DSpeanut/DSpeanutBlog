<!DOCTYPE HTML>
<!--
	Eunji Noh Portfolio
	https://dspeanut.github.io/DSpeanutBlog/
-->
<html>
	<head>
		<title>Eunji Noh's Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="index.html" class="logo">
									<span class="symbol"><img src="images/Main logo/peanut logo.png" alt="" /></span><span class="title">EUNJI NOH</span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="index.html">Home</a></li>
							<li><a href="Rottentomatoes.html">Project 1: Movie Analaysis project</a></li>
							<li><a href="E-commerce.html">Project 2: E-commerce customer modling project</a></li>
							<li><a href="Drybean.html">Project 3: Multiclass classification and model comparison project</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Multiclass classification and model comparison</h1>
							<span class="image main"><img src="images/Drybean/Beans_post.png" alt="" /></span>
							<p><h2>Multilclass classification and model comparison:<br/>
								Tree-based vs Deep learning models on drybean dataset</h2>
							
								The main goal of this project is to build the classification models using both tree-based and deep learning models to evaluate predictive model performance. 
								The dataset used for this project is <b>Dry Bean Dataset</b> from <a href="https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset">UCI Machine Learning Repository</a>
								<br/><br/>
								From this analysis, I would like to explore and find the answers to below questions:
								<br/>
								   <ol><li>1. Building a model for multiclass classification</li>
									   <li>2. Does deep learning model perform better than tree-based model?</li>
									   <li>3. Which model should be used?</li>
									</ol><hr /></p>

							<p><h3>Project skill goals</h3>
							<ul><li>Explaroty Data Analysis</li>
							<li>Data Wrangling</li>
							<li>Data Visualization</li>
							<li>Data Modeleling(Decision tree and Deep learning models)
							<li>Hyperparameter tuning</li></ul></p>
							
							
							<p><h3>Used tools</h3>
								<ul><li>R: Skimr, Tree, Caret, RandomForest, Gbm, dplyr, ggplot2, GGally</li>
								<li>Python : Numpy, Pandas, Matplotlib, Tensorflow, Sklearn</li>
							<hr />
							<br/>

							<h2><u>Part 1</u>　 Exploratory Data Analysis</h2>
							
							<p><center><span class="image"><img src="images/Drybean/data description.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Table 1.</b> Data summary</figcaption></font></center></p>
	
							<p>The dataset contains a total of 17 variables and 13,611 observations. 
								Most of the variables are representing the shape of the bean such as length, diameter, 
								on the other hand, some of the variables are created by combining two variables such as 'Aspect Ratio'(Combination of MajorAxisLength and MinorAxisLength). 
								The dataset include integer, float, and string types.
								Finally, the dependent variable of the model is ‘Class’ which is divided into seven classes of beans: 
								‘Barbunya’,‘Bombay’, ‘Cali’, ‘Dermason’, ‘Horoz’, ‘Seker’, ‘Sira’. </p>
							
								<p><script src="https://gist.github.com/DSpeanut/1c2da023a06cb630e6ccbaff22b5e9ca.js"></script></p>


							<p>First of all, all the necessary packages are loaded into R. Using 'Skimr' package I checked the summary of data and how it's formed.</p>

							<p><center><span class="image"><img src="images/Drybean/targetvariable.png.jpeg" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 1.</b> Dependent variable analysis</figcaption></font></center></p>
							<p> The dependent variable has 7 classes and each class has a different size. 
								Among seven classes, Dermason was the largest class with 26% followed by Sira(19%), Seker(14%), and Horoz(14%). 
								These four classes take over approximately 70% of the dataset. 
								The smallest size of the class was Bombay around 4%.</p>
	
							<p><center><span class="image fit"><img src="images/Drybean/All.jpg" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 2.</b> Independent variable analysis</figcaption></font></center></p>

							<p>
							Independent variables were plotted for analysis and some interesting findings are listed below:
								<ol><li>Data follows a normal distribution in spite of slight skewness in most variables.</li>
								<li>The class ‘Bombay’ has a slightly different pattern to the other classes in the following variables: Area, Perimeter, MajorAxisLength, MinorAxisLength, ConvexArea, EquivDiameter, ShapeFactor1.
								For example, in ‘Area’ variable while the mean values of other classes are small and similar to each other, Bombay’s mean value is relatively greater and separated from others.</li>
								<li>Area, Perimeter, MajorAxisLength, MinorAxisLength variables are all highly correlated.</li>
								<li>Some of the variables' scatterplots show the seven classes are well separated.</li></ol>
							</p>
								

							<h2><u>Part 2</u>　 Data Modeling</h2>
							<h3>I. Tree-based models</h3>
							<h4>1. General Decision Tree Model</h4>

							<p><center><span class="image fit"><img src="images/Drybean/general decision tree.jpeg" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 3.</b> General Decision Tree</figcaption></font></center></p>
							<p>The problem I would like to solve with this dataset is the ‘multi-class classification problem’ and the trees were built. 
								During the tree modeling process , it will select the variable that gives the best split which is either the lowest Gini index or Entropy. 
								After repeating this process several times until no further gain can be made or any other pre-set stopping rules are satisfied.
								Before modeling, the dataset was divided into two groups for the performance testing: train-data (10,889 observations) and test-data (2,722 observations). 
								Throughout this assignment, all models were trained using train-data and tested for their performances by comparing predicted values and real values with a confusion matrix.
								Given the dataset, the general tree was built and plotted as figure 3. The tree size was 11 and a depth of 6. 
								Seven variables out of 16 were used as internal nodes : MajorAxisLength , MinorAxisLength , Perimeter , Compactness, ConvexArea, ShapeFactor1, and roundness. 
								These seven variables are considered to be the most important features for the fitted trees . 
								Adjusting the stopping rule such as changing minimum observation in each section will allow to use of all the 16 variables and also may increase tree size and training accuracy, 
								however it can also lead to overfitting.</p>

							
							<h4>2. Pruned Tree Model</h4>

							<p><center><span class="image fit"><img src="images/Drybean/pruned_tree.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 4.</b> Mis-class level by pruned tree-size(left)/Pruned tree with size of 7(right)</figcaption></font></center></p>
							<p>One of the ways to prevent overfitting the tree is ‘Pruning’. Pruning identify the least reliable branches and remove them to decrease generalization error. 
								The pruning process was tested to check if test accuracy can be increased . 
								The result of pruning tree was evaluated with the number of misclassifications based on the pruned tree size (Figure 4 left chart). 
								The tree size 7 was finally selected and the tree was plotted as figure 4(right).
								The pruned tree had a size of 7 and it only used 4 variables (MajorAxisLength, MinorAxisLength, Compactness, and ShapeFactor) for the splitting. 
								The training accuracy was 84.98% and the testing accuracy was 84.35%. <br/></p>
								
								<p><center><span class="image"><img src="images/Drybean/generalvsprune.png" alt="" /></span></center>
									<center><figcaption><font size = "3"><b>Table 2.</b> Comparison between a general tree and a pruned tree on test/training accuracy</figcaption></font></center></p>	
								<p>Unexpectedly, the pruned tree had a lower accuracy on both train-data and test-data than the previous tree. 
								Normally pruning can reduce the training accuracy and increase the test accuracy. 
								However, in this case, it can be assumed that the tree was possibly over-pruned, thus it became a too simple tree to produce good accuracies considering the large-size of data.
								 As it can be seen in figure 4, the right chart shows an overly pruned tree size of 7 and a depth of 5. 
								Therefore, pruning was not very effective to get high accuracy of the general decision tree in this case. 
								However, changing the stopping rule of the general tree might be helpful to get a higher test accuracy.</p>


							<h4>3. RandomForest Model</h4>
							<p>Given the train-data, random forest model was trained with subset selections of a total of 16 independent variables, and variable importance was plotted as figure 5.</p>
							<p><center><span class="image"><img src="images/Drybean/featureimportance.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 5.</b> Feature Importance by Random Forest model</figcaption></font></center></p>
							<p>Figure  5  illustrates  the  feature  importance  by  MeanDecreaseAccuracy  and  MeanDecreaseGini.  
								MeanDecreaseAccuracy represents how much decrease will occur if the corresponding variable is not included in the tree. 
								The mean decrease in the Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. 
								In both measurements, ‘ShapeFactor1’ and ‘MajorAxisLength’ were in the top 3 important variables. 
								The top 10 variables out of 16 appear to make a relatively large impact on the accuracy.<br/></p>

							<p><center><span class="image"><img src="images/Drybean/Randomforesttuning.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 6.</b> Test accuracy by the number of subset variables selected (left) / the number of trees(right)</figcaption></font></center></p>
							<p> As a further exploration , subset variable numbers and the number of trees were measured by cross - validation and the results are plotted in figure 6. 
								In terms of tree numbers, between the 20 to 100 trees the model produced similar accuracies range in 92%~93%. 
								Only the random-forest model with tree-size below 10 had poor accuracy around 89%. 
								The result of subset variable number testing is plotted(Figure 6 right).Unlike the number of trees testing , the subset variable testing did not show a clear pattern to choose the optimal value.
								The maximum test accuracy was from choosing 54 trees and subset variable number 9. 
								With this result, the random-forest model was tested again resulting in train accuracy of 99%, test accuracy of 92%.<br/></p>

							<h4>3. Boosting Model</h4>

							<p>For boosting model, there are three parameters to tune, 1) the number of trees, 2) the shrinkage parameter, 3) the number of splits in each tree. 
								These three parameters were tested to identify the optimal parameter values to get the highest test accuracy.</p>
							
							<p><center><span class="image"><img src="images/Drybean/Boosting_numberoftrees.jpeg" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 7.</b> Test accuracy by the number of trees (boosting model)</figcaption></font></center></p>
							
							<p>The optimal number of trees was identified by cross-validation on test accuracy. (See figure 7) 
								Since the testing focused on the number of trees, the shrinkage was fixed at 0.01 and the interaction depth was fixed at 1. 
								As the graph shows, the model between 0 to 100 trees had relatively low test accuracies and the graph increased steadily from 200 to 800.
								After 800 to 1,000, no significant improvement was found. Therefore, the optimal number of trees was selected as 800.</p>

							<p><center><span class="image"><img src="images/Drybean/boostingmodel.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Table 3.</b> Comparison between boosting tree interaction depth and shrinkage for test accuracy</figcaption></font></center></p>
							
							<p>After the number of trees was selected, the Interaction depth and shrinkage tuning were conducted. 
								Overall, when shrinkage was 0.01 test accuracy was higher than shrinkage of 0.001. 
								Between the Interaction depth 1 and 2, the Interaction depth 2 outperformed the depth of 1. 
								By training the optimal boosting model with 800 trees, 0.01 shrinkage, and 2 interaction depth, train accuracy has resulted in 93.92% and test accuracy was 92.84%. </p>
	
								
							<p><center><span class="image"><img src="images/Drybean/treemodels.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Table 4.</b> Comparison between three tree-based models</figcaption></font></center></p>
							
							<p>Between five different tree-based models, as shown in Table 4. 
								There was a significant increase from the general decision tree model to the tuned random-forest model 
								by nearly 6% and the boosting model was considered as the best model with the highest test accuracy of 92.84% </p>
		
							<h3>I. Deep Neural Network Models</h3>	

							<p>Deep learning is a type of machine learning technique that learns the pattern of the unstructured data and learns the process to find the function to solve either ‘regression’ or ‘classification’ problems. 
							The deep learning model is consisting of a node layer, units, weights. Depending on the neural network type the network can be divided into single-layer perceptron(SLP) or multi-layer perceptron(MLP). 
							The idea of a neural network came from the human brain. 
							These neural network components are functioning similarly to the human brain and can be trained like any other ML algorithm.</p>
							<p>There are various neural network types which can be chosen depending on type of problem and the dataset. 
							Most basic neural network is Artificial neural network (ANN) and Recurrent Neural Network (RNN) is often used for music composition, human action recognition. 
							Long/Short-Term Memory (LSTM) is used for speech recognition or machine translation. 
							For this project, ANN model was chosen considering the data type and the size. The final model will be tuned by testing various hyper-parameters. 
							7 hyper-parameters were tested to choose the final model: 
							1) The number of hidden units, 2) The number of hidden layers, 3) The number of epochs, 4) The number of batch-size, 
							5) Activation functions, 6) Optimizers. 7) Regularization. 
							While one parameter is being tested other parameters were fixed as same for accurate comparison.</p>


								<br/>
								<br/>
								<br/>
								<br/>
								<br/>
								<br/>
								<br/>
								<br/>
								<p>The dataset is the customer data from e-commerce website and it contains customer purchasing and activity history information. 
									The dataframe consists of total 8 variables (3 Categorical variables and 5 Numerical variables). 
									The data has total 500 observations.</p>
								<p><center><span class="image"><img src="images/E-commerce/[Resized]originaldf.png" alt="" /></span></center>
									<center><figcaption><font size = "3"><b>Table 2.</b> First 5 rows of the dataset</figcaption></font></center></p>
		
								<p>  Table 2 shows some statistical values of each numerican variable. 
									On average customers spent 33 minutes per each session and people tend to stay longer on the website compared to the application. 
									The customer spent minimum $256 upto $765 and $499 on average.
									The email and address are not included in the variable selection for the modeling. 
									However, I wanted to see if ‘Avatar’ information might have some impact on the customer purchase trend.
								<br/>
								 The email and address are not included in the variable selection for the modeling. 
								However, I wanted to see if ‘Avatar’ information might have some impact on the customer purchase trend.</p>
								<p><center><span class="image"><img src="images/E-commerce/avatar.png" alt="" /></center>
									<center><figcaption><font size = "3"><b>Table 3.</b> The correlation coefficients of variable 'Avatar'</figcaption></font></center></p>
								<p>The table 3 shows that the avatar features have insignificant relationships with the ‘Yearly Amount Spent’ variable. 
									Therefore, this variable is not included in the modeling variables.</p> 

								<p><center><span class="image"><img src="images/E-commerce/[Resized]pairplot.png" alt="" /></center>
								<center><figcaption><font size = "3"><b>Figure 1.</b> Paired plots of numerical variables</figcaption></font></center></p>
								<p> The paired plot indicates all numerical variables follow a normal distribution. 
									From the plots it looks like the variable ‘Length of membership’ is highly correlated with the target variable ‘Year Amount Spent’.
									 Also, ‘Time on App’ looks slightly correlated. </p>
									
								<script src="https://gist.github.com/DSpeanut/4c9c8bcf5c858aaabdcb67a27ff72441.js"></script>
								<p>To confirm if all numerical variables follow normal distributions, I have conducted **Shapiro-Wilk Test** for normality test.
									 All test results did not reject H0, therefore all numerical variables follow normal distributions.
									 The columns 'Email', 'Address', 'Avatar' are removed, then I checked the correlation coefficients of variables using seaborn's heatmap.</p>
									 
								
								<script src="https://gist.github.com/DSpeanut/0817b4339673352becf86644223ed832.js"></script>
								<br/>
								<center>
									<span class="image"><img src="images/E-commerce/Correlation coefficients.png" alt="" /></center></span>
									 <center><figcaption><font size = "3"><b>Figure 2.</b> Correlation coefficients heatmap</figcaption></font></center>
								<br/>
								<p>The heatmap confirm that the variable <b>'Length of Membership'</b> has a strong relationship with the <b>'Year Amount Spent'</b> as its coefficient is 0.81. 
									The variable <b>'Time on App'</b>'s correlation coefficient is 0.5 which is considered to be a strong positive relationship. 
									<b>'Avg. Session length'</b> has a moderate positive relationship. On the other hand, unlike other variables 
									<b>'Time on Website'</b> has a negative correlation coefficient. Since the value is nearly 0, this variable has almost no relationship to the target variable.</p>
									<p>There are several ways to evaluate a predictive model performance. For this project The Pareto Principle and 10-fold cv methods were used.</p> 
									<p>
									1. The Pareto Principle :  It's also called 80-20 rules. The dataset will be split into 80% and 20%. 
									80% of data will be used for training the model and 20% of data will be used for evaluation of model performance.</br>
									2. K-fold cross validation : Split the dataset into k groups and select one group as a hold out(test set) and train the model on rest of groups(l-1 groups). 
									Repeat the process 10 times to use all unique group as a test set. Then calcualte the average of the result of all iterations. </p>
								
								<h2><u>Part 2</u>　 Linear Regression Model</h2>
								<script src="https://gist.github.com/DSpeanut/2a6d79a14772cf7b28ae261da909f3bd.js"></script>
									 
								<p>The data-set is split into 80% and 20%. Then model was trained and tested accordingly</p>
								<center>
									<span class="image"><img src="images/E-commerce/linearresult.png" alt="" /></center></span>
									 <center><figcaption><font size = "3"><b>Table 4.</b> The correlation coefficients of variable 'Avatar'</figcaption></font></center>

								<br/>	
								<p>The linear model is fitted on the train-data. Each variable has coefficients as it shown table 4.</p>
								
								The homoscedasticity was checked with the residual plot.


								<script src="https://gist.github.com/DSpeanut/cf1b22aae743bc2bc6adf47c7ce57317.js"></script>


								<center>
									<span class="image"><img src="images/E-commerce/Residual plot.png" alt="" /></center></span>
									 <center><figcaption><font size = "3"><b>Figure 3.</b> Residual plot</figcaption></font></center>
								<p> Residual plot shows there is no particular pattern in the predicted values using the linear model. Therefore, Fitted linear regression model satisfy the homoscedasticity assumption</p>

								<script src="https://gist.github.com/DSpeanut/4602abdfb774fbb43d660cdfae77fc80.js"></script>

								To evaluate the performance, linear model was used to predict 'Year Amount Spent' based on the test data(x variables). 
								Table 5 represent the performace result using both 'The Pareto Principle' and '10-fold cross-validation' methods.</p>
								
								<center>
									<span class="image"><img src="images/E-commerce/comp.png" alt="" /></center></span>
									 <center><figcaption><font size = "3"><b>Table 5.</b> Model performance on test data</figcaption></font></center>

								
								<script src="https://gist.github.com/DSpeanut/375a5e2ffd45bebbeb4d5691c7c066e1.js"></script>

								<p>Since the linear model is built, I have tested the model to predict 'Yearly Spent Amount' for two new customers.

								<blockquote> 
									<b>Input</b><br/>
									Customer 1 : 35,15,30,2 <br/>
									Customer 2 : 40,20,20,4 <br/><br/>
									
									<b>Output</b><br/>
									Customer1's　yearly purchase amount is approximately $568<br/>
									Customer2's　yearly purchase amount is approximately $1013</blockquote>

								<h2><u>Part 3</u>　 Logistic Regression Model</h2>


								<p>In part 3, I would like to build a multiple logistic regression model to predict the customer class.
								<ul><li>Independent variables : 'Avg. Session Length', 'Time on App', 'Time on Website','Length of Membership'</li>
								<li>Dependent variable : 'Customer class(VIP, Regular)'</li></ul></p>
								
								Since 'year amount spent' was used to divide the customer class, it was not included in the independent variables.</p>

								<script src="https://gist.github.com/DSpeanut/fc8e9eadd46f6e854eb7d44c0742ef6a.js"></script>
								<p><center><span class="image"><img src="images/E-commerce/Classification.png" alt="" /></span></center>
									<center><figcaption><font size = "3"><b>Table 6.</b> Assigning classification</figcaption></font></center></p>
		
								<p> The average yearly purchased amount by customer is $499.31. I simply divided the customer classes into 2 categories:</p>
									<li>Class 1 : Customer who yearly spent amount is > $499</li>
									<li>Class 0 : Customer who Yearly spent amount is < $499</li></p>

								<p><center><span class="image"><img src="images/E-commerce/Correlation coefficients by customer class.png" alt="" /></span></center>
									<center><figcaption><font size = "3"><b>Figure 4.</b> Paired plots by customer class</figcaption></font></center></p>

								<p> The pairplot shows clear division between two different classe on the variables: 'Yearly Amount Spent', 'Length of Membership'. However, 'Time on App', 'Time on Website', and 'Avg. Session Length' variables did not seem to have a clear division line.</p>
								<script src="https://gist.github.com/DSpeanut/f209123279d445aaf05820329eb3d22f.js"></script>
								<p><center><span class="image"><img src="images/E-commerce/Logistic Regression Plots.png" alt="" /></center>
									<center><figcaption><font size = "3"><b>Figure 5.</b> Logistic regression plots</figcaption></font></center></p>

								<p>Using seabron package, the logistic regression graphs were plotted for all variables. 
									Corresponding to the pairplot result, figure 5 shows that logistic regression were well fitted on all variables except 'Time on website'. 
									Meaning the customers were not able to be divided by 2 classes using 'Time on website' variable.</p>
								
								<script src="https://gist.github.com/DSpeanut/bf2cad5fc84aed7e27e057e7030c43d3.js"></script>
								
								<p><center><span class="image"><img src="images/E-commerce/logistic result.png" alt="" /></center>
									<center><figcaption><font size = "4"><b>Figure 6.</b> Logistic regression result</figcaption></font></center></p>
								<p>The multiple logistic regression was trained and tested. The model has a 95% accuracy of classification. 
									As it can be seen in confusion matrix, out of 125 customers 119 customers were correctly classified
									(60 regular customers and 59 VIPs) and 6 customers were incorrectly classified.</p>
								
								<script src="https://gist.github.com/DSpeanut/8e880a6e3baee5a8eb84c44ebbbe5623.js"></script>

								<p> As a last step, the multiple logistic regression model was used to predict customer's class with new data points. </p>
			
								<blockquote> 
									<b>Input</b><br/>
									New customer: 30, 15.265884, 35, 4.5 <br/>
									
									<b>Output</b><br/>
									The new customer's yearly purchase amount is approximately $604<br/>
									The customer is class VIP</blockquote>
								

								<center><b><font size = "10"> CONCLUSION </font></b></center>

								<p>With this project, I successfully built multiple linear, logistic regression models to predict customer yearly spent amount and their customer class.
									One interesting finding was that Time spent on app was important factor on customer purchase, wherease Time spent on website did not have a great impact.
									This fact tells us that the company should focus on making a good quality and convinient application rather than the website.
									Additionally, The company now can utilize these two models to identify target customers for various marketing promotions. 
								</p><br/>
									
						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<section>
								<h2>Get in touch</h2>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<input type="text" name="name" id="name" placeholder="Name" />
										</div>
										<div class="field half">
											<input type="email" name="email" id="email" placeholder="Email" />
										</div>
										<div class="field">
											<textarea name="message" id="message" placeholder="Message"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send" class="primary" /></li>
									</ul>
								</form>
							</section>
							<section>
								<h2>Follow</h2>
								<ul class="icons">
									<li><a href="#" class="icon brands style2 fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="#" class="icon brands style2 fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon brands style2 fa-instagram"><span class="label">Instagram</span></a></li>
									<li><a href="#" class="icon brands style2 fa-dribbble"><span class="label">Dribbble</span></a></li>
									<li><a href="#" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="#" class="icon brands style2 fa-500px"><span class="label">500px</span></a></li>
									<li><a href="#" class="icon solid style2 fa-phone"><span class="label">Phone</span></a></li>
									<li><a href="#" class="icon solid style2 fa-envelope"><span class="label">Email</span></a></li>
								</ul>
							</section>
							<ul class="copyright">
								<li>&copy; Untitled. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>