<!DOCTYPE HTML>
<!--
	Eunji Noh Portfolio
	https://dspeanut.github.io/DSpeanutBlog/
-->
<html>
	<head>
		<title>Eunji Noh's Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="index.html" class="logo">
									<span class="symbol"><img src="images/Main logo/peanut logo.png" alt="" /></span><span class="title">EUNJI NOH</span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>


				<!-- Menu -->
				<nav id="menu">
					<h2>Menu</h2>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="Rottentomatoes.html">Project 1: Rotten Tomatoes Movie Analaysis</a></li>
						<li><a href="Retail.html">Project 2: Retail Sales Analysis</a></li>
						<li><a href="Sustainability.html">Project 3: The world map of sustainability</a></li>
						<li><a href="E-commerce.html">Project 4: E-commerce customer classification</a></li>
						<li><a href="Drybean_tree-based.html">Project 5: Multiclass Classification and Model Comparison (part 1)</a></li>
						<li><a href="Drybean_deeplearning.html">Project 6: Multiclass Classification and Model Comparison (part 2)</a></li>
						<li><a href="https://devpost.com/software/alteryx-olympics-data">Project 7: The 120 years of Olympics</a></li>
					</ul>
				</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Multiclass classification and model comparison</h1>
							<span class="image main"><img src="images/Drybean/Beans_post.png" alt="" /></span>
							<h2>Multiclass classification and model comparison:<br/>
								Part 1 : Multiclass classification/Parameter tuning for tree-based models.</h2>
								<center><p><b>Full code available <a href="https://github.com/DSpeanut/Projects/blob/main/Multiclass%20classification_tree-based%20models.R"> here </a></b></center></p>
								<p>							
								The main goal of this project is to build classification models using both tree-based and deep learning models to evaluate predictive model performance. 
								The dataset used for this project is <b>Dry Bean Dataset</b> from <a href="https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset">UCI Machine Learning Repository</a>
								<p><b>The project is divided into 2 posts Part 1 and Part 2.</b><br/></p>
								
								<ul><li>Part 1 : Multiclass classification for tree-based models(Modeling, Parameter tuning, Comparison)</li></ul>
								<ul><li>Part 2 : Multiclass classification for Deep Neural Network models(Modeling, Parameter tuning, Comparison, Conclusion)</li></ul>
								

								<br/>
								From this analysis, I would like to explore and find the answers to below questions:
								<br/>
								   <ol><li>Building a model for multiclass classification</li>
									   <li>Does a deep learning model perform better than a tree-based model?</li>
									   <li>Which model should be used?</li>
									</ol><hr /></p>

							<p><h3>Project skill goals</h3>
							<ul><li>Exploratory Data Analysis</li>
							<li>Data Wrangling</li>
							<li>Data Visualisation</li>
							<li>Data Modelling(Decision tree and Deep learning models)
							<li>Hyperparameter tuning</li></ul></p>
							
							
							<p><h3>Used tools</h3>
								<ul><li>R: Skimr, Tree, Caret, RandomForest, Gbm, dplyr, ggplot2, GGally</li>
								<li>Python : Numpy, Pandas, Matplotlib, Tensorflow, Sklearn</li>
							<hr />
							<br/>

							<h2><u>Part 1</u>　 Exploratory Data Analysis</h2>
							
							<p><center><span class="image"><img src="images/Drybean/data description.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Table 1.</b> Data summary</figcaption></font></center></p>
	
							<p>The dataset contains a total of 17 variables and 13,611 observations. 
								Most of the variables are representing the shape of the bean such as length, diameter, 
								on the other hand, some of the variables are created by combining two variables such as 'Aspect Ratio'(Combination of MajorAxisLength and MinorAxisLength). 
								The dataset include integer, float, and string types.
								Finally, the dependent variable of the model is ‘Class’ which is divided into seven classes of beans: 
								‘Barbunya’,‘Bombay’, ‘Cali’, ‘Dermason’, ‘Horoz’, ‘Seker’, ‘Sira’. </p>
							
							<p>First of all, all the necessary packages are loaded into R. Using 'Skimr' package I checked the summary of data and how it's formed.</p>

							<p><center><span class="image"><img src="images/Drybean/targetvariable.png.jpeg" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 1.</b> Dependent variable analysis</figcaption></font></center></p>
							<p> The dependent variable has 7 classes and each class has a different size. 
								Among seven classes, Dermason was the largest class with 26% followed by Sira(19%), Seker(14%), and Horoz(14%). 
								These four classes take over approximately 70% of the dataset. 
								The smallest size of the class was Bombay around 4%.</p>
	
							<p><center><span class="image fit"><img src="images/Drybean/All.jpg" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 2.</b> Independent variable analysis</figcaption></font></center></p>

							<p>
							Independent variables were plotted for analysis and some interesting findings are listed below:
								<ol><li>Data follows a normal distribution in spite of slight skewness in most variables.</li>
								<li>The class ‘Bombay’ has a slightly different pattern to the other classes in the following variables: Area, Perimeter, MajorAxisLength, MinorAxisLength, ConvexArea, EquivDiameter, ShapeFactor1.
								For example, in ‘Area’ variable while the mean values of other classes are small and similar to each other, Bombay’s mean value is relatively greater and separated from others.</li>
								<li>Area, Perimeter, MajorAxisLength, MinorAxisLength variables are all highly correlated.</li>
								<li>Some of the variables' scatterplots show the seven classes are well separated.</li></ol>
							</p>
								

							<h2><u>Part 2</u>　 Data Modeling</h2>
							<h3>I. Tree-based models</h3>
							<h4>1. General Decision Tree Model</h4>

							<p><center><span class="image fit"><img src="images/Drybean/general decision tree.jpeg" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 3.</b> General Decision Tree</figcaption></font></center></p>
							<p>The problem I would like to solve with this dataset is the ‘multi-class classification problem’ and the trees were built. 
								During the tree modeling process , it will select the variable that gives the best split which is either the lowest Gini index or Entropy. 
								After repeating this process several times until no further gain can be made or any other pre-set stopping rules are satisfied.
								Before modeling, the dataset was divided into two groups for the performance testing: train-data (10,889 observations) and test-data (2,722 observations). 
								Throughout this assignment, all models were trained using train-data and tested for their performances by comparing predicted values and real values with a confusion matrix.
								Given the dataset, the general tree was built and plotted as figure 3. The tree size was 11 and a depth of 6. 
								Seven variables out of 16 were used as internal nodes : MajorAxisLength , MinorAxisLength , Perimeter , Compactness, ConvexArea, ShapeFactor1, and roundness. 
								These seven variables are considered to be the most important features for the fitted trees . 
								Adjusting the stopping rule such as changing minimum observation in each section will allow to use of all the 16 variables and also may increase tree size and training accuracy, 
								however it can also lead to overfitting.</p>

							
							<h4>2. Pruned Tree Model</h4>

							<p><center><span class="image fit"><img src="images/Drybean/pruned_tree.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 4.</b> Mis-class level by pruned tree-size(left)/Pruned tree with size of 7(right)</figcaption></font></center></p>
							<p>One of the ways to prevent overfitting the tree is ‘Pruning’. Pruning identify the least reliable branches and remove them to decrease generalization error. 
								The pruning process was tested to check if test accuracy can be increased . 
								The result of pruning tree was evaluated with the number of misclassifications based on the pruned tree size (Figure 4 left chart). 
								The tree size 7 was finally selected and the tree was plotted as figure 4(right).
								The pruned tree had a size of 7 and it only used 4 variables (MajorAxisLength, MinorAxisLength, Compactness, and ShapeFactor) for the splitting. 
								The training accuracy was 84.98% and the testing accuracy was 84.35%. <br/></p>
								
								<p><center><span class="image"><img src="images/Drybean/generalvsprune.png" alt="" /></span></center>
									<center><figcaption><font size = "3"><b>Table 2.</b> Comparison between a general tree and a pruned tree on test/training accuracy</figcaption></font></center></p>	
								<p>Unexpectedly, the pruned tree had a lower accuracy on both train-data and test-data than the previous tree. 
								Normally pruning can reduce the training accuracy and increase the test accuracy. 
								However, in this case, it can be assumed that the tree was possibly over-pruned, thus it became a too simple tree to produce good accuracies considering the large-size of data.
								 As it can be seen in figure 4, the right chart shows an overly pruned tree size of 7 and a depth of 5. 
								Therefore, pruning was not very effective to get high accuracy of the general decision tree in this case. 
								However, changing the stopping rule of the general tree might be helpful to get a higher test accuracy.</p>


							<h4>3. RandomForest Model</h4>
							<p>Given the train-data, random forest model was trained with subset selections of a total of 16 independent variables, and variable importance was plotted as figure 5.</p>
							<p><center><span class="image"><img src="images/Drybean/featureimportance.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 5.</b> Feature Importance by Random Forest model</figcaption></font></center></p>
							<p>Figure  5  illustrates  the  feature  importance  by  MeanDecreaseAccuracy  and  MeanDecreaseGini.  
								MeanDecreaseAccuracy represents how much decrease will occur if the corresponding variable is not included in the tree. 
								The mean decrease in the Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. 
								In both measurements, ‘ShapeFactor1’ and ‘MajorAxisLength’ were in the top 3 important variables. 
								The top 10 variables out of 16 appear to make a relatively large impact on the accuracy.<br/></p>

							<p><center><span class="image"><img src="images/Drybean/Randomforesttuning.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 6.</b> Test accuracy by the number of subset variables selected (left) / the number of trees(right)</figcaption></font></center></p>
							<p> As a further exploration , subset variable numbers and the number of trees were measured by cross - validation and the results are plotted in figure 6. 
								In terms of tree numbers, between the 20 to 100 trees the model produced similar accuracies range in 92%~93%. 
								Only the random-forest model with tree-size below 10 had poor accuracy around 89%. 
								The result of subset variable number testing is plotted(Figure 6 right).Unlike the number of trees testing , the subset variable testing did not show a clear pattern to choose the optimal value.
								The maximum test accuracy was from choosing 54 trees and subset variable number 9. 
								With this result, the random-forest model was tested again resulting in train accuracy of 99%, test accuracy of 92%.<br/></p>

							<h4>3. Boosting Model</h4>

							<p>For boosting model, there are three parameters to tune, 1) the number of trees, 2) the shrinkage parameter, 3) the number of splits in each tree. 
								These three parameters were tested to identify the optimal parameter values to get the highest test accuracy.</p>
							
							<p><center><span class="image"><img src="images/Drybean/Boosting_numberoftrees.jpeg" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Figure 7.</b> Test accuracy by the number of trees (boosting model)</figcaption></font></center></p>
							
							<p>The optimal number of trees was identified by cross-validation on test accuracy. (See figure 7) 
								Since the testing focused on the number of trees, the shrinkage was fixed at 0.01 and the interaction depth was fixed at 1. 
								As the graph shows, the model between 0 to 100 trees had relatively low test accuracies and the graph increased steadily from 200 to 800.
								After 800 to 1,000, no significant improvement was found. Therefore, the optimal number of trees was selected as 800.</p>

							<p><center><span class="image"><img src="images/Drybean/boostingmodel.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Table 3.</b> Comparison between boosting tree interaction depth and shrinkage for test accuracy</figcaption></font></center></p>
							
							<p>After the number of trees was selected, the Interaction depth and shrinkage tuning were conducted. 
								Overall, when shrinkage was 0.01 test accuracy was higher than shrinkage of 0.001. 
								Between the Interaction depth 1 and 2, the Interaction depth 2 outperformed the depth of 1. 
								By training the optimal boosting model with 800 trees, 0.01 shrinkage, and 2 interaction depth, train accuracy has resulted in 93.92% and test accuracy was 92.84%. </p>
	
								
							<p><center><span class="image"><img src="images/Drybean/treemodels.png" alt="" /></span></center>
								<center><figcaption><font size = "3"><b>Table 4.</b> Comparison between three tree-based models</figcaption></font></center></p>
							
							<p>Between five different tree-based models, as shown in Table 4. 
								There was a significant increase from the general decision tree model to the tuned random-forest model 
								by nearly 6% and the boosting model was considered as the best model with the highest test accuracy of 92.84% </p>
		


								<center><b><font size = "10"> To be continued in <a href="Drybean_deeplearning.html">Part 2</a></font></b></center>

									
						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<section>
								<h2>Get in touch</h2>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<input type="text" name="name" id="name" placeholder="Name" />
										</div>
										<div class="field half">
											<input type="email" name="email" id="email" placeholder="Email" />
										</div>
										<div class="field">
											<textarea name="message" id="message" placeholder="Message"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send" class="primary" /></li>
									</ul>
								</form>
							</section>
							<section>
								<h2>Follow</h2>
								<ul class="icons">
									<li><a href="#" class="icon brands style2 fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="#" class="icon brands style2 fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="#" class="icon brands style2 fa-instagram"><span class="label">Instagram</span></a></li>
									<li><a href="#" class="icon brands style2 fa-dribbble"><span class="label">Dribbble</span></a></li>
									<li><a href="#" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="#" class="icon brands style2 fa-500px"><span class="label">500px</span></a></li>
									<li><a href="#" class="icon solid style2 fa-phone"><span class="label">Phone</span></a></li>
									<li><a href="#" class="icon solid style2 fa-envelope"><span class="label">Email</span></a></li>
								</ul>
							</section>
							<ul class="copyright">
								<li>&copy; Untitled. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>